{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cLNnYbIHJSOR",
        "TdwiOwSBfsxj",
        "yZ7cBBoYI449",
        "e6aok9dWikaA",
        "bQJVuIHpp9Qu",
        "tg9p7wHDfGcZ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riddars/Dream_team/blob/main/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22Copy_of_%D0%A5%D0%B0%D0%BA%D0%B0%D1%82%D0%BE%D0%BD_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKM5ee8xhiIS",
        "outputId": "5c6e7094-a8d5-4898-a758-1535ce9db6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.65.0)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.1)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.56.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->shap) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lt5f_RDDVH-W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import shap\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Загрузка данных из CSV файлов\n",
        "data_df = pd.read_csv('https://raw.githubusercontent.com/Riddars/Hakaton/main/data.csv', index_col=0)\n",
        "bacterial_descriptors_df = pd.read_csv('https://raw.githubusercontent.com/Riddars/Hakaton/main/bacterial_descriptors.csv')\n",
        "drug_descriptors_df = pd.read_csv('https://raw.githubusercontent.com/Riddars/Hakaton/main/drug_descriptors.csv', index_col=0)"
      ],
      "metadata": {
        "id": "H5CWbhX2XIPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Смотрим шапки датасетов"
      ],
      "metadata": {
        "id": "KvRjLxMpt5Nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Была предпринята попытка хаполнить пропуски в данных. Окончательным успехом она не увенчаалсь. В данных очень много пропусков.\n",
        "Есть два метода MIC, MBC почему-то они не требуют знания концентрации NP и дозы лекарства. (~25% data)\n",
        "\n",
        "\n",
        "Для нужных бактерий нужно найти какие они грам-положительные или грам-отрицательные. Это важно по статье которую нашла Настя, ну и из общих соображений \"очевидно\""
      ],
      "metadata": {
        "id": "Ar__8MOqamTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Основной датасет"
      ],
      "metadata": {
        "id": "cLNnYbIHJSOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.head(2)"
      ],
      "metadata": {
        "id": "S36fumqGaQuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()"
      ],
      "metadata": {
        "id": "9V8gG2DuOyxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# в каких колонках есть пропуски\n",
        "\n",
        "data_df.isna().sum(), data_df.isna().sum()/len(data_df)*100"
      ],
      "metadata": {
        "id": "yT7AtGmazScj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## for preprocessing results\n",
        "\n",
        "\n",
        "def data_df_fix_column_types(df=data_df):\n",
        "    start_len = len(data_df)\n",
        "    data_df[\"shape\"] = data_df[\"shape\"].astype(\"category\")\n",
        "    data_df[\"method\"] = data_df[\"method\"].astype(\"category\")\n",
        "\n",
        "    # 32+ насколько оно больше непонятно, при этом из всего 10 - можно пожертвовать\n",
        "    # Escherichia coli\t, Actinobacillus pleuropneumoniae\n",
        "    # chem_synthesis_reduction_by_D_maltose\n",
        "    data_df.drop(\n",
        "        data_df[data_df.ZOI_drug_NP == '32+'].index, inplace=True)\n",
        "    data_df.drop(\n",
        "        data_df[data_df.ZOI_drug == '32+'].index, inplace=True)\n",
        "    data_df['ZOI_drug'] = data_df.ZOI_drug.astype(float)\n",
        "\n",
        "    # 50+ замечено 7 штук\n",
        "    data_df.drop(\n",
        "        data_df[data_df.ZOI_NP == '50+'].index,inplace=True)\n",
        "    # # 4000 <- перед ними последнее значение это 220. Из всего 3\n",
        "    # data_df.drop(\n",
        "    #     data_df[data_df.ZOI_NP == '4000'].index,inplace=True)\n",
        "    data_df[\"ZOI_NP\"] = data_df.ZOI_NP.astype(float)\n",
        "\n",
        "\n",
        "    # 17+2 <- это вероятно 17 +/- 2\n",
        "    # data_df.drop(\n",
        "    #     data_df[data_df.ZOI_drug_NP == '17+2'].index, inplace=True)\n",
        "    data_df[\"ZOI_drug_NP\"] = data_df[\"ZOI_drug_NP\"].replace('17+2', 17)\n",
        "    data_df[\"ZOI_drug_NP\"] = data_df.ZOI_drug_NP.astype(float)\n",
        "\n",
        "    # NP_concentration 170/85/*\n",
        "    '''\n",
        "    если посмотреть на:\n",
        "    mask_ = (data_df.NP_concentration.str.contains(\"170/85/\")) | (data_df.NP_concentration == '42.5')\n",
        "    data_df[mask_].sort_values(\"Bacteria\").set_index(\"Bacteria\")\n",
        "    такое преобразование выглядит разымным\n",
        "    '''\n",
        "    data_df[\"NP_concentration\"] = (data_df[\"NP_concentration\"]\n",
        "                                        .apply(lambda x : float(str(x).split('/')[-1]))\n",
        "                                )\n",
        "    finish_len = len(data_df)\n",
        "    print(\"Было удалено :\", start_len - finish_len, \"строк\")\n",
        "\n",
        "\n",
        "data_df_fix_column_types(data_df)"
      ],
      "metadata": {
        "id": "sa-_YK82SCZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# эксперименты только с наночастицами\n",
        "mask1_ = (data_df.ZOI_drug.isna()) & (data_df.ZOI_drug_NP.isna()) & (~data_df.ZOI_NP.isna()) & (~data_df.NP_concentration.isna())\n",
        "data_df.loc[mask1_, 'Drug_dose'] = 0\n",
        "data_df.loc[mask1_, ['Drug', 'Drug_class_drug_bank']] = ['No', 'No']\n",
        "data_df.loc[mask1_, \"ZOI_drug\"] = 0\n",
        "data_df.loc[mask1_, \"ZOI_drug_NP\"] = data_df.loc[mask1_, \"ZOI_NP\"]\n",
        "\n",
        "# эксперименты только с лекарствами\n",
        "mask2_ = (data_df.ZOI_NP.isna()) & (data_df.ZOI_drug_NP.isna()) & (~data_df.ZOI_drug.isna()) & (~data_df.Drug_dose.isna())\n",
        "data_df.loc[mask2_] # а тут ноль экземпляров"
      ],
      "metadata": {
        "id": "hk-jmhi5edK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.isna().sum()"
      ],
      "metadata": {
        "id": "B6xnU6e9XFsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## изучаем датасет бактерий"
      ],
      "metadata": {
        "id": "ZHd2EohzKR8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Вспомогательный датасет 2 (бактерии)"
      ],
      "metadata": {
        "id": "mNLRM4XvuHq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bacterial_descriptors_df.head(2)"
      ],
      "metadata": {
        "id": "4XrQJ9uJaW9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# смотрим каких названий из data_df нет в bacterial_descriptors\n",
        "set(data_df['Bacteria'].unique()) - set(bacterial_descriptors_df['Bacteria'].unique())\n",
        "\n",
        "# а вот избыточных много\n",
        "# set(bacterial_descriptors_df['Bacteria'].unique()) - set(data_df['Bacteria'].unique())"
      ],
      "metadata": {
        "id": "PN3RJcx_Q0M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.replace('Bacillus spp.        ', 'Bacillus sp.', inplace=True)\n",
        "data_df.replace('Acinetobacter baumanii', 'Acinetobacter baumannii', inplace=True)\n",
        "data_df.replace('Actinobacillus pleuropneumoniae ', 'Actinobacillus pleuropneumoniae', inplace=True)\n",
        "# data_df.replace('Candida glochares', 'Candida glochares', inplace=True) # ??? заменить само на себя?\n",
        "\n",
        "data_df.replace('Enterobacter cloacae ', 'Enterobacter cloacae', inplace=True)\n",
        "data_df.replace('Salmonella  typhi', 'Salmonella typhi', inplace=True)\n",
        "data_df.replace('Salmonella Paratyphi', 'Salmonella enterica', inplace=True) # https://www.ncbi.nlm.nih.gov/datasets/taxonomy/54388/\n",
        "# data_df.replace('Serratia marcescens', 'Serratia marcescens', inplace=True)  # ??? заменить само на себя?\n",
        "\n",
        "\n",
        "# unrecoverable\n",
        "data_df.drop(\n",
        "    data_df[\n",
        "        (data_df.Bacteria == 'Serratia marcescens') | ((data_df.Bacteria == 'Candida glochares'))\n",
        "    ].index,\n",
        "    inplace=True\n",
        ")"
      ],
      "metadata": {
        "id": "c_KGw370S_2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fix some grams\n",
        "bacterial_descriptors_df[bacterial_descriptors_df.gram.isna()].Bacteria.value_counts()\n",
        "\n",
        "bacterial_descriptors_df.loc[\n",
        "    bacterial_descriptors_df.Bacteria.isin(\n",
        "        ['Candida albicans',\n",
        "         'Candida glabrata',\n",
        "         'Candida saitoana',\n",
        "         'Micrococcus luteus',\n",
        "        'Staphylococcus aureus'\n",
        "        'Staphylococcus epidermidis',\n",
        "         'Bacillus subtilis',\n",
        "         'Bacillus sp.',\n",
        "         ])\n",
        ", 'gram'\n",
        "] = 'p'\n",
        "\n",
        "bacterial_descriptors_df.loc[\n",
        "    bacterial_descriptors_df.Bacteria.isin(\n",
        "[\n",
        "            'Neisseria mucosa',\n",
        "            'Escherichia coli',\n",
        "            'Salmonella enterica',\n",
        "            'Salmonella typhi',\n",
        "            'Klebsiella pneumonia',\n",
        "            'Pseudomonas aeruginosa',\n",
        "            'Vibrio parahaemolyticus',\n",
        "            'Serratia odorifera',\n",
        "            'Pseudomonas koreensis',\n",
        "            'Haemophilus influenzae',\n",
        "         ])\n",
        ", 'gram'\n",
        "] = 'n'\n"
      ],
      "metadata": {
        "id": "iJZDb9_xUkz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вспомогательный датасет 3 (антибиотики)"
      ],
      "metadata": {
        "id": "jbh7ewaMuLcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drug_descriptors_df.head(2)"
      ],
      "metadata": {
        "id": "2lnSjLSEaZdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Смотрим описание основного датасета"
      ],
      "metadata": {
        "id": "79pEMeQWuB5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()"
      ],
      "metadata": {
        "id": "kH5Ec17r-wNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "объединяем дф основной и дф бактерий и дф артибиотиков"
      ],
      "metadata": {
        "id": "VrLVDHP9u0Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = data_df.merge(bacterial_descriptors_df, how='left', on='Bacteria')"
      ],
      "metadata": {
        "id": "82h9AXeg-UF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.merge(drug_descriptors_df, how='left', left_on='Drug', right_on='drug')\n",
        "\n",
        "# set(df['Drug'].unique()) - set(df['Drug'].unique()), df[\"Drug\"].value_counts()[\"Neomycin\"]\n",
        "\n",
        "df.drop(\n",
        "    df[df.Drug == 'Neomycin'].index, inplace= True\n",
        ")"
      ],
      "metadata": {
        "id": "ryyMWNH8JDZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8pb0Ws3bS4rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature инжиринг"
      ],
      "metadata": {
        "id": "drKzMxnHLOhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "bacteria_taxonomy_descriptors = [\n",
        "    'kingdom', 'subkingdom', 'clade',\n",
        "    'phylum', 'class', 'order', 'family', 'genus', 'Bacteria']\n",
        "# добавить расстояния на графе таксономии\n",
        "def make_graph_better(bacterial_descriptors_df=bacterial_descriptors_df):\n",
        "    '''\n",
        "    Строит филогенетический граф\n",
        "    '''\n",
        "    tax_df = bacterial_descriptors_df.loc[:,bacteria_taxonomy_descriptors] # копирует\n",
        "    gg_ = nx.Graph()\n",
        "    for top_node in tax_df['kingdom']: # чтобы был связным\n",
        "        gg_.add_edge('root', top_node)\n",
        "\n",
        "    for parents, childs in zip(bacteria_taxonomy_descriptors, bacteria_taxonomy_descriptors[1:]):\n",
        "        tax_df[childs] = tax_df.loc[:, [childs]].fillna(childs) # уникальный NaN для кадждого столбца\n",
        "        edge_lst = tax_df[[parents, childs]].values\n",
        "        gg_.add_edges_from(edge_lst)\n",
        "\n",
        "    # gg_.remove_node(np.nan)\n",
        "    # nx.draw_spring(gg_)\n",
        "    return gg_\n",
        "\n",
        "\n",
        "def get_distances(bacterial_descriptors_df, df):\n",
        "    '''Берёт из графа расстояния и отфильтровывает только нужные\n",
        "    '''\n",
        "    g_= make_graph_better(bacterial_descriptors_df)\n",
        "    lengths = dict(nx.all_pairs_shortest_path_length(g_))\n",
        "    rv = {key: lengths[key] for key in df.Bacteria.unique()}\n",
        "    return rv\n",
        "\n",
        "\n",
        "def populate_distances_to_df(bacterial_descriptors_df=bacterial_descriptors_df,\n",
        "                             df=df):\n",
        "    df = df.copy()\n",
        "    distances = get_distances(bacterial_descriptors_df, df)\n",
        "    for bacteria in distances.keys():\n",
        "        df['d_to_' + bacteria] = df.Bacteria.map(distances[bacteria])\n",
        "    # return df.loc[:, df.columns.str.startswith('d_to_')] # посмотреть на расстояния\n",
        "    return df\n",
        "\n",
        "def remove_bacteria_taxonomy_descriptors_from_df(df):\n",
        "    df.drop(columns=bacteria_taxonomy_descriptors, inplace=True)\n",
        "\n",
        "df = populate_distances_to_df(bacterial_descriptors_df, df)\n",
        "remove_bacteria_taxonomy_descriptors_from_df(df)"
      ],
      "metadata": {
        "id": "z9yQoXoWLTFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_fold_increase_in_antibacterial_activity(df):\n",
        "    df.drop(columns='fold_increase_in_antibacterial_activity (%)', inplace=True)\n",
        "\n",
        "drop_fold_increase_in_antibacterial_activity(df)"
      ],
      "metadata": {
        "id": "3fZfiEZOTfgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_gram_to_valeus(df):\n",
        "    df['gram'] = df.gram.map({'p': 1, 'n': -1})\n",
        "    # .astype(int)\n",
        "\n",
        "convert_gram_to_valeus(df)"
      ],
      "metadata": {
        "id": "8k6bumvIZdAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### попытка нарисовать филогенетический граф с количеством семплов на каждый вид"
      ],
      "metadata": {
        "id": "sXdWPWfNPFNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # как нарисовать дерево https://stackoverflow.com/questions/57512155/how-to-draw-a-tree-more-beautifully-in-networkx\n",
        "# # вариант 2: https://plotly.com/python/tree-plots/\n",
        "# tree_ = make_graph_better()\n",
        "# leafes = [x for x in tree_.nodes() if tree_.degree(x) == 1]\n",
        "# weights = df.Bacteria.value_counts()\n",
        "# for l in leafes:\n",
        "#     tree_.nodes[l]['weight'] = weights[l]\n",
        "\n",
        "# # nx.draw_circular(tree_)"
      ],
      "metadata": {
        "id": "ylvsGpmwMxog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выкидываем первый столбик"
      ],
      "metadata": {
        "id": "e_mPutyCvDzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.iloc[:, 1:]"
      ],
      "metadata": {
        "id": "B8m8ysPLK3Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Убираем лишние столбцы"
      ],
      "metadata": {
        "id": "641NvePpvBi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# наоборот - какие колонки выбросить\n",
        "\n",
        "df = df.drop(\n",
        "    columns=[\n",
        "#  'Drug',\n",
        " 'Drug_class_drug_bank',\n",
        "#  'Drug_dose',\n",
        "#  'NP_concentration',\n",
        "#  'NP size_min',\n",
        "#  'NP size_max',\n",
        "#  'NP size_avg',\n",
        "#  'shape',\n",
        "#  'method',\n",
        "#  'ZOI_drug',\n",
        "#  'ZOI_NP',\n",
        "#  'ZOI_drug_NP',\n",
        "#  'fold_increase_in_antibacterial_activity (%)',\n",
        "#  'MDR_check',\n",
        "   'Tax_id',\n",
        "#  'kingdom',\n",
        "#  'subkingdom',\n",
        "#  'clade',\n",
        "#  'phylum',\n",
        "#  'class',\n",
        "#  'order',\n",
        "#  'family',\n",
        "#  'genus',\n",
        "#  'species',\n",
        "#  'gram',\n",
        "#  'min_Incub_period, h',\n",
        "#  'avg_Incub_period, h',\n",
        "#  'max_Incub_period, h',\n",
        "#  'growth_temp, C',\n",
        "#  'biosafety_level',\n",
        " 'isolated_from',\n",
        " 'drug',\n",
        " 'chemID',\n",
        " 'prefered_name',\n",
        " 'smiles'\n",
        " ]\n",
        "    )\n",
        "\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "9aZxt4OINX4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "OmNLVeU_k-7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing all the libraries needed\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(df[['NP size_min', 'NP size_avg', 'NP size_max']].corr(), annot = False, xticklabels=True)"
      ],
      "metadata": {
        "id": "jeFEQEeWpZBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2T1duZCLr8YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data cleaning"
      ],
      "metadata": {
        "id": "TdwiOwSBfsxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # приведение всех значений столбца к типу string\n",
        "# data_df['ваш столбец'] = data_df['ваш столбец'].astype(str)\n",
        "\n",
        "# # Проверка наличия отсутствующих значений\n",
        "# missing_values = data_df.isnull().sum()\n",
        "# # Удаление строк с хотя бы одним пропущенным значением\n",
        "# data_df.dropna(inplace=True)\n",
        "\n",
        "# # Посмотреть кол-во дубликатов в столбце\n",
        "# duplicates_count = data_df['ваш столбец'].duplicated().sum()\n",
        "# print('Количество дубликатов:', duplicates_count)\n",
        "# # удаление дубликатов\n",
        "# data_df.drop_duplicates(subset='ваш столбец', keep='first', inplace=True)\n",
        "\n",
        "# # Подсчет количества уникальных элементов в таблице\n",
        "# unique_count = df['Столбец'].nunique()\n",
        "# print('Количество уникальных элементов:', unique_count)\n",
        "\n",
        "# # Удаление ненужных столбцов\n",
        "# df = df.drop(columns=['ваш столбец'])\n",
        "\n",
        "# # Заполнение пустых значений\n",
        "# df = df.fillna('')\n",
        "\n",
        "# # Удаление строк с выбросами\n",
        "# data = data[(data['NP size_avg'] >= 0) & (data['NP size_avg'] <= 50)]\n",
        "\n",
        "# # Исправление опечаток\n",
        "# data['Bacteria'] = data['Bacteria'].str.replace('Escherichia coli', 'E. coli')\n",
        "\n",
        "# # Фильтрация по условию\n",
        "# filtered_data = data[data['Drug_class_drug_bank'] == 'Lactams']"
      ],
      "metadata": {
        "id": "yJsM195nqYuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# УДАЛЕНИЕ ВЫБРОСОВ"
      ],
      "metadata": {
        "id": "yZ7cBBoYI449"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #удалим выбросы по интерквартельному размаху\n",
        "# def IQR_data_clean(df, colname):\n",
        "#     filter = df[colname]\n",
        "#     Q1, Q3 = filter.quantile(0.25), filter.quantile(0.75)\n",
        "#     IQR = Q3 - Q1\n",
        "#     dfr = df[\n",
        "#         ~(\n",
        "#             (filter < (Q1 - 1.5 * IQR)) |(filter > (Q3 + 1.5 * IQR))\n",
        "#         )\n",
        "#     ]\n",
        "#     return dfr"
      ],
      "metadata": {
        "id": "XrZlihSeMnkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat = r'''\n",
        " /\\_/\\\n",
        "( o.o )\n",
        " > ^ <\n",
        "'''\n",
        "\n",
        "print(cat)"
      ],
      "metadata": {
        "id": "68obPjkQMoqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_df.NP_Synthesis.unique()"
      ],
      "metadata": {
        "id": "niThl5stjA_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anastasiia's\n"
      ],
      "metadata": {
        "id": "e6aok9dWikaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# f, axes = plt.subplots(3, 3)\n",
        "# sns.distplot(df['Drug_dose'], kde=True, ax = axes[0,0])\n",
        "# sns.distplot(df['NP size_min'], kde=True, ax = axes[0,1])\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "_jCzgKIpimOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DF2 bacterial_descriptors_df"
      ],
      "metadata": {
        "id": "bQJVuIHpp9Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bacterial_descriptors_df[['gram', 'species']].drop_duplicates()"
      ],
      "metadata": {
        "id": "SdqYesDZq4Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o9xMeMhAqZ0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Функция split - разделение на тест и трейн"
      ],
      "metadata": {
        "id": "lp8AfcnhfGcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#пока сделаем OneHot\n",
        "df = df.drop(columns = ['ZOI_NP'])\n",
        "condition = ((df['method'] == 'disc_diffusion') | (df['method'] == 'well_diffusion'))\n",
        "df1 = df[condition]\n",
        "df2 = df[~condition]\n",
        "df1=df1.dropna()\n",
        "df5=pd.get_dummies(df1)\n",
        "df5.info()"
      ],
      "metadata": {
        "id": "1Q6qJkD9eQ7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"ZOI_drug_NP\"\n",
        "def split(df, target):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = target,axis=1), df[target], test_size=0.3, random_state=42)\n",
        "  cols = df.columns.tolist()\n",
        "  columns_to_remove = [target]\n",
        "  cols = list(filter(lambda x: x not in columns_to_remove, cols))\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  scaler = MinMaxScaler()\n",
        "  sclr = scaler.fit(X_train)\n",
        "  X_train = pd.DataFrame(scaler.transform(X_train), columns=cols)\n",
        "  X_test = pd.DataFrame(scaler.transform(X_test), columns=cols)\n",
        "  return (X_train, X_test, y_train, y_test)\n",
        "X_train, X_test, y_train, y_test = split(df5,target)"
      ],
      "metadata": {
        "id": "nHsQX4rEfGcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Функция Random Forest"
      ],
      "metadata": {
        "id": "tg9p7wHDfGcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import  mean_squared_error\n",
        "def RF(X_train, y_train, X_test, y_test):\n",
        "  regr = RandomForestRegressor(random_state=0, n_estimators=174, max_depth=15, oob_score = True)\n",
        "  regr.fit(X_train, y_train)\n",
        "  explainer = shap.TreeExplainer(regr)\n",
        "  shap_values = explainer(X_train)\n",
        "  predicted = regr.predict(X_test)\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.scatter(y_test, predicted, edgecolors=(0, 0, 0))\n",
        "  ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
        "  ax.set_xlabel('Measured')\n",
        "  ax.set_ylabel('Predicted')\n",
        "  plt.show()\n",
        "\n",
        "  mse = mean_squared_error(y_test, predicted)\n",
        "  rmse = np.sqrt(mse)\n",
        "\n",
        "   # Рассчет RMSE в процентах\n",
        "  rmse_percentage = (rmse / np.mean(y_test)) * 100\n",
        "\n",
        "  # Вывод результатов\n",
        "  print(\"RMSE:\", rmse)\n",
        "  print(\"RMSE в процентах:\", rmse_percentage)\n",
        "  print(r2_score(y_test, predicted))\n",
        "  shap.summary_plot(shap_values, X_train, plot_type='bar')\n",
        "  return(regr)"
      ],
      "metadata": {
        "id": "KrWSILc6fGcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = RF(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "7gDUAIl8kDS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.info()"
      ],
      "metadata": {
        "id": "Je7z3IBXmhSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Что пытался делать **Арсен**"
      ],
      "metadata": {
        "id": "RqJig1ECIok4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('https://raw.githubusercontent.com/Riddars/Hakaton/main/Final(no).csv', index_col=0)"
      ],
      "metadata": {
        "id": "boWwSbfHGsJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# # Удаление строк с пропущенными значениями\n",
        "# df.dropna(inplace=True)\n",
        "# # Разделение данных на признаки (X) и целевую переменную (y)\n",
        "# X = df[['NP_concentration', 'NP size_min', 'NP size_max', 'ZOI_drug', 'MDR_check']]\n",
        "# y = df['ZOI_drug_NP']\n",
        "# # Шкалирование признаков с использованием стандартного масштабирования\n",
        "# scaler = StandardScaler()\n",
        "# X_scaled = scaler.fit_transform(X)\n",
        "# # Импьютация пропущенных значений с использованием средних значений\n",
        "# imputer = SimpleImputer(strategy='mean')\n",
        "# X_imputed = imputer.fit_transform(X_scaled)\n",
        "\n",
        "# # Разделение данных на обучающий и тестовый наборы\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "m3CfZRYIGqGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Импорт необходимых библиотек\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# from sklearn.metrics import r2_score\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# # Разделение данных на признаки (X) и целевую переменную (y)\n",
        "# X = df[['NP_concentration', 'NP size_min', 'NP size_max', 'ZOI_drug', 'MDR_check']]\n",
        "# y = df['ZOI_drug_NP']\n",
        "\n",
        "# # Разделение данных на обучающий и тестовый наборы\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Создание и обучение модели линейной регрессии\n",
        "# model = LinearRegression()\n",
        "# model.fit(X_train, y_train)\n",
        "\n",
        "# # Предсказание на тестовом наборе\n",
        "# y_pred = model.predict(X_test)\n",
        "\n",
        "# # Вычисление среднеквадратичной ошибки (MSE)\n",
        "# mse = mean_squared_error(y_test, y_pred)\n",
        "# print('Среднеквадратичная ошибка:', mse)\n",
        "\n",
        "# # Рассчитываем коэффициент детерминации\n",
        "# r2 = r2_score(y_test, y_pred)\n",
        "# print('Коэффициент детерминации (R^2):', r2)\n",
        "\n",
        "# # Визуализация рассеяния фактических значений и предсказанных значений\n",
        "# plt.scatter(y_test, y_pred)\n",
        "# plt.xlabel('Фактические значения')\n",
        "# plt.ylabel('Предсказанные значения')\n",
        "# plt.title('Рассеяние: Фактические значения vs Предсказанные значения')\n",
        "# plt.show()\n",
        "\n",
        "# # Визуализация линии регрессии\n",
        "# plt.plot(y_test, y_test, color='red', label='Идеальное соответствие')\n",
        "# plt.plot(y_test, y_pred, 'o', color='blue', label='Линия регрессии')\n",
        "# plt.xlabel('Фактические значения')\n",
        "# plt.ylabel('Предсказанные значения')\n",
        "# plt.title('Линия регрессии: Фактические значения vs Предсказанные значения')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "rKrPMQKZGjgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Загрузка данных из CSV файла\n",
        "# data = df\n",
        "\n",
        "# # Разделение на признаки и целевую переменную\n",
        "# X = data.drop('ZOI_drug_NP', axis=1)\n",
        "# y = data['ZOI_drug_NP']\n",
        "\n",
        "# # Масштабирование признаков\n",
        "# scaler = StandardScaler()\n",
        "# X_scaled = scaler.fit_transform(df.drop(columns=[\"species\", \"genus\", \"family\", \"order\", \"class\", \"kingdom\", \"phylum\", \"ZOI_drug_NP\"]))\n",
        "\n",
        "# # Разделение на обучающую и тестовую выборки\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Создание модели нейронной сети\n",
        "# model = keras.Sequential([\n",
        "#     keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "#     keras.layers.Dropout(0.2),\n",
        "#     keras.layers.Dense(64, activation='relu'),\n",
        "#     keras.layers.BatchNormalization(),\n",
        "#     keras.layers.Dense(1)\n",
        "# ])\n",
        "\n",
        "# # Компиляция модели с метрикой accuracy\n",
        "# model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "# # Обучение модели\n",
        "# history = model.fit(X_train, y_train, epochs=100, validation_split=0.2)\n",
        "\n",
        "# # Оценка модели на тестовой выборке\n",
        "# loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "# # Вывод результатов\n",
        "# print('Test Loss:', loss)\n",
        "# print('Test Accuracy:', accuracy)\n",
        "\n",
        "# # Построение графика обучения\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('Model Loss')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "kUCezIkPGvVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# # Создание экземпляра OneHotEncoder\n",
        "# encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# # Преобразование категориальных признаков методом OneHotEncoding\n",
        "# encoded_features = encoder.fit_transform(df[categorical_features])\n",
        "\n",
        "# # Получение имен категорий\n",
        "# feature_names = encoder.get_feature_names_out(categorical_features)\n",
        "\n",
        "# # Создание датафрейма из преобразованных признаков с префиксом \"encoded_\"\n",
        "# encoded_df = pd.DataFrame(encoded_features, columns=[f'encoded_{name}' for name in feature_names])\n",
        "\n",
        "# # Переименование столбцов в encoded_df (если необходимо)\n",
        "# encoded_df = encoded_df.rename(columns={'old_name': 'new_name'})\n",
        "\n",
        "# # Объединение преобразованных признаков с исходным датафреймом\n",
        "# df = pd.concat([df, encoded_df])\n",
        "\n",
        "# # Удаление исходных категориальных признаков\n",
        "# df = df.drop(columns=categorical_features)"
      ],
      "metadata": {
        "id": "jyTHpbXEG08z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}